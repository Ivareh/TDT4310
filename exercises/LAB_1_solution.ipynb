{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "## 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task 1.1\n",
    "sent = \"My first car (1974 Ford Pinto) was a trash-can on wheels...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple ways we could think of to tokenize this text. Handling parentheses and hyphens are the things to discuss here.\n",
    "\n",
    "I would probably consider (, ) and - as separate tokens, as the token (1974 doesn't really make sense. I would also consider splitting the hyphenated words, but as this doesn't occur too often, leaving it as-is is probably fine."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2\n",
    "If we entered this, we would probably want to predict \"was\" or perhaps a comma. It's unlikely you'd encounter the parenthesis in a typical text after the word \"car\". The key here is to consider domains of text. This format could be used in a car magazine, for example. \n",
    "\n",
    "Also, the tense of *car* in \"my first car\" will change depending on the words preceding it. \"This is my first car\" (present) while starting the sentence with \"My first car\" is almost guaranteed to be in past tense. Thus, \"is\" will very likely be a wrong answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3\n",
    "Next-word predicting is inherently difficult. The clue is to consider context, but as you'll see, this is far from trivial, and can in no way be represented by the last few words preceding the future word. Context is often found throughout paragraphs, and even entire documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'first', 'car', '(', '1974', 'Ford', 'Pinto', ')', 'was', 'a', 'trash-can', 'on', 'wheels', '...']\n"
     ]
    }
   ],
   "source": [
    "# tokenize using nltk:\n",
    "tokens = nltk.word_tokenize(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('My', 'first'), ('first', 'car'), ('car', '('), ('(', '1974'), ('1974', 'Ford'), ('Ford', 'Pinto'), ('Pinto', ')'), (')', 'was'), ('was', 'a'), ('a', 'trash-can'), ('trash-can', 'on'), ('on', 'wheels'), ('wheels', '...')]\n"
     ]
    }
   ],
   "source": [
    "print(list(nltk.ngrams(tokens, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('My', 'first', 'car'), ('first', 'car', '('), ('car', '(', '1974'), ('(', '1974', 'Ford'), ('1974', 'Ford', 'Pinto'), ('Ford', 'Pinto', ')'), ('Pinto', ')', 'was'), (')', 'was', 'a'), ('was', 'a', 'trash-can'), ('a', 'trash-can', 'on'), ('trash-can', 'on', 'wheels'), ('on', 'wheels', '...')]\n"
     ]
    }
   ],
   "source": [
    "print(list(nltk.ngrams(tokens, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3\n",
    "Large n-values will yield very specific predictions tailored to the data the model has already seen. While this may not be too bad (especially if you train it on _your_ data), it will not generalize well. A small n-value will yield more general predictions, but might be less accurate. There is no such thing as an ideal value for n, but depending on the data sources, you may want to limit it to 2-3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({('that', 'that'): 2, ('that', 'is'): 2, ('is', 'that'): 2, ('is', 'not'): 2, ('not', 'is'): 2, ('is', 'is'): 1, ('that', 'it'): 1, ('it', 'it'): 1, ('it', 'is'): 1, ('is', '.'): 1})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk bag of words:\n",
    "sent_2 = \"That that is is that that is not is not is that it it is.\".lower()\n",
    "bag = nltk.FreqDist(nltk.ngrams(nltk.word_tokenize(sent_2), 2))\n",
    "bag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'is': 6, 'that': 5, 'not': 2, 'it': 2})"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_2_lower = sent_2.lower()\n",
    "# task 1: create vocabulary\n",
    "tokens_2 = nltk.word_tokenize(sent_2_lower)\n",
    "tokens_2 = [w for w in tokens_2 if w.isalpha()]\n",
    "bow = nltk.FreqDist(tokens_2)\n",
    "bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 6, 2, 2]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# consider only the values for each index:\n",
    "list(bow.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1\n",
    "This representation is extremely simple, but may be used in documents where the vocabulary isn't too large - or with a specified keywords that you deem to be important. The fact that the word \"is\" and \"that\" occurs often is probably not very unique. As we'll see in the next lab, TF-IDF is a better approach to this problem, as it evaluates the importance of a word in a document based on its term frequencies and the number of documents it occurs in.\n",
    "\n",
    "## 3.2\n",
    "You can compare other sentences (with the same vocabulary) using e.g. the cosine similarity. This, however, disregards the count for each word, as these are merely scalars. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "5c7b89af1651d0b8571dde13640ecdccf7d5a6204171d6ab33e7c296e100e08a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
